# aa-energi-2025

**Project information in English:** [README-ENGLISH.md](README-ENGLISH.md)

----

# 1. MLP Sinus-Cosinus-regresjon

En rask demonstrasjon av bruk av **PyTorch** for Ã¥ tilpasse en 2D sinus-cosinus-funksjon ved hjelp av to forskjellige Multi-Layer Perceptron (MLP)-arkitekturer:

- **Liten MLP** (moderat kapasitet)  
- **Stor MLP** (hÃ¸y kapasitet, for Ã¥ overtilpasse og fange opp fine detaljer)

![Sinus Cos Overfitting Demo](assets/img/1-sincos.png)

# **2. Tidsserieprognoser med smÃ¥ datasett â€“ Krig mot autoregressoren**  

**_Vi forsÃ¸ker Ã¥ utfordre klassiske autoregressive modeller ved hjelp av dyp lÃ¦ring pÃ¥ et lite datasettâ€”og det var en tung kamp._**  

![Prediksjonstidslinje](assets/img/2_predicted.png)  

----

# 1. MLP Sinus-Cosinus-regresjon

En rask demonstrasjon av bruk av **PyTorch** for Ã¥ tilpasse en 2D sinus-cosinus-funksjon ved hjelp av to forskjellige Multi-Layer Perceptron (MLP)-arkitekturer:

- **Liten MLP** (moderat kapasitet)  
- **Stor MLP** (hÃ¸y kapasitet, for Ã¥ overtilpasse og fange opp fine detaljer)

![Sinus Cos Overfitting Demo](assets/img/1-sincos.png)

## Oversikt

1. **Data**  
   Vi har $(x, i)$-par og et mÃ¥l $z = \sin(\cos(x)) + \sin(\cos(i))$.  
   - Inndata er lagret i `data/X_sincos.txt`
   - MÃ¥lverdier er lagret i `data/y_sincos.txt`
   - Data levert av Ã… Energi.

2. **Modeller**  
   - **SmallMLP**: 2 â†’ 100 â†’ 100 â†’ 1 (med ReLU-aktiveringer)  
   - **LargeMLP**: 2 â†’ 100 â†’ 500 â†’ 500 â†’ 100 â†’ 1 (med ReLU-aktiveringer)

3. **Trening**  
   - Vi bruker **MSE Loss** og **Adam Optimizer** i PyTorch.  
   - **LargeMLP** er bevisst overparameterisert for Ã¥ tilpasse (og til og med overtilpasse) dataene svÃ¦rt godt.

4. **Resultater**  
   - Vi sammenligner den sanne funksjonen med prediksjoner fra begge MLP-ene i en 3D-plot.

![Sinus Cos Overfitting Demo](assets/img/1-sincos.png)

> *Fra venstre til hÃ¸yre*:  
> **(1)** Sann funksjon  
> **(2)** Prediksjoner fra liten MLP  
> **(3)** Prediksjoner fra stor MLP

## Kom i gang

1. **Installer avhengigheter**  
   ```bash
   pip install torch matplotlib numpy
   ```
2. **KjÃ¸r scriptet**  
   ```bash
   python 1-sincos.py
   ```
   - Juster hyperparametere (epoker, lÃ¦ringsrate) om Ã¸nskelig.

3. **Plotting**  
   Scriptet viser automatisk et 3D-overflateplot for Ã¥ sammenligne prediksjonene.

## LÃ¦rdommer

- **SmÃ¥ vs. store modeller**: Et stÃ¸rre nettverk kan tilnÃ¦rme mÃ¥lfunksjonen svÃ¦rt nÃ¸yaktig, men kan overtilpasse dersom datagrunnlaget er begrenset.  
- **Visualisering**: 3D-overflateplott hjelper oss med Ã¥ visuelt vurdere hvor godt modellen fanger opp den underliggende funksjonen.  
- **PyTorch**: Viser hvor enkelt det er Ã¥ bygge og trene MLP-er pÃ¥ egendefinerte data med bare noen fÃ¥ linjer Python-kode.

----


# **2. Tidsserieprognoser med smÃ¥ datasett â€“ Krig mot autoregressoren**  

**_Vi forsÃ¸ker Ã¥ utfordre klassiske autoregressive modeller ved hjelp av dyp lÃ¦ring pÃ¥ et lite datasettâ€”og det var en tung kamp._**  

![Prediksjonstidslinje](assets/img/2_predicted.png)  

## **Oversikt**  

Dette prosjektet utforsker forskjellige tilnÃ¦rminger til Ã¥ forutsi en daglig tidsserie med kun **~4000 observasjoner** (etter Ã¥ ha tatt hensyn til lag-funksjoner). MÃ¥let var Ã¥ undersÃ¸ke om moderne dyp lÃ¦ringâ€”LSTMer og Transformereâ€”kunne overgÃ¥ klassiske statistiske metoder i en situasjon med lite data.  

Vi testet fire modeller:  

1. **Naiv AR**: Den enkleste baselinenâ€”antar at dagens verdi vil vÃ¦re den samme som i gÃ¥r.  
2. **AR(5)**: En lineÃ¦r autoregressiv modell som bruker de siste fem dagene for Ã¥ forutsi neste dag.  
3. **LSTM**: Et rekurrent nevralt nettverk trent pÃ¥ sekvenser av 30 dager.  
4. **Transformer**: En selvoppmerksomhetsmodell som ogsÃ¥ bruker et 30-dagers vindu.  

### **Vant dyp lÃ¦ring?**  

Ikke denne gangen. Med bare noen fÃ¥ tusen datapunkter og kun tre eksogene variabler (`x1, x2, x3`), slet de nevrale nettverkene med Ã¥ finne meningsfulle mÃ¸nstre. De autoregressive modellene, spesielt AR(5), presterte betydelig bedre fordi:  

- Datasettet er **svÃ¦rt lite** (~4000 rader), noe som begrenser lÃ¦ringskapasiteten til dype modeller.  
- De eksogene variablene har **svak forklaringskraft**, noe som betyr at de ikke bidrar mye til prognosen.  
- Tidsserien i seg selv er **sterkt autoregressiv**, noe som betyr at tidligere verdier alene gir et sterkt prediktivt signalâ€”noe de enklere AR-modellene hÃ¥ndterer godt.  

## **Endelige resultater**  

| Modell       | MAE  | MSE  |  
|-------------|------|------|  
| **Naiv AR**  | **2.626**  | **19.377**  |  
| **AR(5)**     | **2.466**  | **17.183**  |  
| LSTM         | 4.930  | 59.427  |  
| Transformer  | 5.853  | 70.126  |  

BÃ¥de LSTM og Transformer ble klart slÃ¥tt av de naive og AR(5)-modellene. De dype lÃ¦ringsmodellene hadde nesten **dobbelt sÃ¥ hÃ¸y MAE** og **tre til fire ganger hÃ¸yere MSE**. En klar seier for den klassiske tilnÃ¦rmingen i dette tilfellet.  

---

## **Viktige grafer**  

### **Prediksjonstidslinje**  

Denne grafen sammenligner faktiske og predikerte verdier over tid. Jo nÃ¦rmere en modells prediksjoner fÃ¸lger de virkelige verdiene, desto bedre presterer den.  

ğŸ“Œ **Hva du bÃ¸r se etter:**  
- Hvilke modeller ligger nÃ¦rmest de faktiske verdiene? Her gjÃ¸r de autoregressive modellene en langt bedre jobb.  
- Henger noen modeller konsekvent etter eller overpredikerer mÃ¥let? Det er ingen systematisk forsinkelse, noe som indikerer at alt er satt opp riktig og at hyperparametrene er rimelige.  
- Hvor mye stÃ¸y introduserer LSTM og Transformer sammenlignet med AR(5)? Svaret er betydelig stÃ¸y og tilfeldige topperâ€”datasettet er for lite til at nevrale nettverk kan skinne!  

![Prediksjonstidslinje](assets/img/2_predicted.png)  

### **Absolutt feil over tid**  

Denne grafen viser hvordan hver modells absolutte feil utvikler seg over tid. Den hjelper med Ã¥ identifisere perioder hvor modellene sliter mest.  

ğŸ“Œ **Hva du bÃ¸r se etter:**  
- Er det spesifikke tidsperioder hvor feilene Ã¸ker kraftig? De stÃ¸rste toppene sammenfaller med store bevegelser i $y$, og fordi datasettet ikke inneholder nok forklaringskraft, gjÃ¸r LSTMer og Transformere store feil.  
- GjÃ¸r Ã©n modell konsekvent stÃ¸rre feil enn de andre?  
- Viser dype lÃ¦ringsmodeller ustabil eller uforutsigbar atferd?  

![Feiltidslinje](assets/img/2_error_timeline.png)  

### **MAE- og MSE-sammenligning**  

Disse sÃ¸ylediagrammene gir en direkte numerisk sammenligning av hvor godt hver modell presterte.  

- **MAE (Mean Absolute Error)** viser gjennomsnittsstÃ¸rrelsen pÃ¥ feilene pÃ¥ en intuitiv mÃ¥te.  
- **MSE (Mean Squared Error)** gir stÃ¸rre vekt til store feil, noe som gjÃ¸r den mer fÃ¸lsom for ekstreme avvik.  

ğŸ“Œ **Hva du bÃ¸r se etter:**  
- AR(5)-modellen oppnÃ¥r lavest MAE og MSEâ€”vinneren av denne utfordringen.  
- LSTM og Transformer har betydelig hÃ¸yere feil, noe som viser at de sliter med det begrensede datasettet.  
- Den naive modellen presterer overraskende godt, noe som viser hvor sterkt autoregressiv tidsserien er.  

**MAE-sammenligning:**  
![MAE](assets/img/2-MAE.png)  

**MSE-sammenligning:**  
![MSE](assets/img/2-MSE.png)  

---

## **Hvordan kjÃ¸re koden**  

1. Installer avhengigheter:  
   ```bash
   pip install numpy pandas matplotlib torch scikit-learn
   ```
2. KjÃ¸r hovedskriptet:  
   ```bash
   python 2-tahps.py
   ```
3. Sjekk konsollutdata og genererte grafer.  

---

## **Konklusjon**  

Til tross for vÃ¥re beste forsÃ¸k, **vant ikke dyp lÃ¦ring denne kampen**â€”men det er ikke overraskende. AR(5) og til og med den naive modellen presterte godt fordi tidligere verdier alene inneholdt nok prediktiv informasjon.  

Imidlertid, i et scenario med **mer data** og **sterkere eksogene variabler**, kunne LSTM og Transformer ha gjort det bedre. ForelÃ¸pig fremhever dette prosjektet en viktig lÃ¦rdom innen tidsserieprognoser: **noen ganger er det enkleste ogsÃ¥ det beste.**  

Vil du eksperimentere? PrÃ¸v Ã¥ legge til flere funksjoner, justere hyperparametere eller bruke forskjellige arkitekturer for Ã¥ se om du kan vippe vektskÃ¥len i favÃ¸r av dyp lÃ¦ring!  

---

ğŸ’¬ **SpÃ¸rsmÃ¥l eller tilbakemeldinger?**  
Ta gjerne kontakt eller opprett en issueâ€”diskuterer alltid gjerne tidsserieprognoser! ğŸš€  

---